---
{"dg-publish":true,"permalink":"/论文解读/模型分析/🧠 Kimi K2 后训练解析：迈向 Agentic Intelligence 的关键步骤/","tags":["gardenEntry"]}
---



> **摘要**  
> Kimi K2 大模型在后训练阶段着重提升其 agentic 能力——即模型自主感知、推理、计划与行动的智能性。我们将重点关注其数据合成、强化学习、系统架构与评估方式，理解 Kimi K2 如何在 agentic 路线上迈出关键一步。

---

## 1. 引言：Agentic 能力为何重要？

当前的大模型正从静态任务能力向 Agentic Intelligence 转型，即具备主动探索环境、制定行动计划、迭代交互与自我评估的能力。这意味着模型不仅是回答者，而是具备自主行为决策与执行能力的智能体（Agent）。

Kimi K2 正是基于这一目标设计，提出：

- 利用**合成环境+真实沙盒**构建大规模 agentic 数据集
    
- 设计多样化**工具调用轨迹**，学习多步任务的策略与反思
    
- 应用**可验证奖励 + 自我批判奖励**结合的 RL 策略
    
- 搭建**高吞吐、容错的 RL 系统架构**
    

---

## 2. 后训练目标与方法总览

后训练（Post-training）是将预训练阶段学到的语言先验转化为**可执行的智能行为**的关键桥梁。Kimi K2 的后训练包括三大阶段：

1. **Supervised Fine-Tuning（SFT）**：
    
    - 高质量、多任务指令微调
        
    - 特别构建 agentic 专属数据集
        
2. **Reinforcement Learning（RL）**：
    
    - 支持多场景 verifiable reward
        
    - 创新性引入 Self-Critique Rubric Reward
        
3. **高效 RL 系统支持**：
    
    - 双引擎架构：训练引擎与推理引擎共存
        
    - 支持多环境 agent rollout 和部分轨迹中断恢复
        

---

## 3. 大规模用于学习tool use的数据合成：从工具到任务再到轨迹
> 💡 为了构建一个**大规模、覆盖多个不同领域**的 SFT 数据集，为不同任务领域定制了一套**数据生成流水线方案**：

1. **响应生成阶段**  
    我们采用 **Kimi 1.5** 以及多个**内部领域专家模型**，针对不同任务生成候选回答。  
    随后，再通过 **大语言模型（LLM）或人工评审**，对这些回答进行自动质量评估与筛选。


2. **数据合成阶段**  
    整体 pipeline 分为以下三个子阶段：
	**a. 工具库构建**：我们首先从真实世界的工具以及由大语言模型合成的工具中，构建了一个大规模的工具规范库，每个工具都包含明确的接口、描述与语义。
	**b. Agent 与任务生成**：针对从工具库中采样得到的每组工具，我们生成一个对应的 agent 来操作这组工具，并为其构建相应的任务目标。
	**c. 轨迹生成**：对于每一组 agent 与任务，我们生成交互轨迹，模拟 agent 多步调用工具完成任务的全过程，包括工具选择、调用顺序、状态追踪与结果验证。
![Pasted image 20250722205805.png](/img/user/Pasted%20image%2020250722205805.png)
### 3.1 工具库构建

 工具库主要有两个来源：

- **MCP工具**：3000+ 来自 GitHub 等开源代码库的真实MCP工具
    
- **合成工具**：通过一种**分层的领域生成流程**合成 20000+ 多样工具，该流程从若干核心类别出发（如金融交易、软件应用、机器人控制等），在每个类别下进一步衍生出多个具体的应用领域。随后，我们为每个应用领域合成专用工具，并为每个工具明确设计其接口、功能描述与操作语义。

> **t‑SNE（t-distributed Stochastic Neighbor Embedding）** 是一种广泛用于**高维数据可视化**的降维算法。它通过保持数据点之间的“邻近关系”，将复杂的高维嵌入压缩到二维或三维空间，便于直观展示数据的聚类与分布。
> 在大语言模型中，一个工具、句子或 token 的表示通常是一个高维向量，我们很难直接理解这些嵌入的空间分布，而 t‑SNE 能让我们将其“投影”为一张 2D 散点图。
![Pasted image 20250722211947.png](/img/user/Pasted%20image%2020250722211947.png)
这里展示的图片分别为MCP 工具与合成工具使用 t‑SNE 可视化在功能空间中的分布，可见合成工具大幅扩展了原有工具集合的覆盖空间


---

### 3.2 多样化 Agent 与任务生成


- **Agent 多样性**：通过合成各种 system prompt以及配备工具仓库中不同的工具组合，生成了数千个不同的agent，也就是说，我们拥有了具有不同能力、专业领域和行为模式的多元化agent group
    
- **任务多样性**：对于每个agent配置，都会生成从简单到复杂的任务。每项任务都配有一个明确的评分标准，该评分标准指定了成功标准、预期的工具使用模式和评估检查点。这种基于评分标准的生成方法 (**Rubric-Based Task Generation**) 可确保对代理绩效进行一致和客观的评估。

---

### 3.3 多轮交互轨迹生成与过滤

采用 multi-agent 模拟生成真实交互轨迹：

- **User Agent**：扮演具有不同的通信风格和偏好用户角色，与完成task agent进行多轮对话，从而创建自然的交互模式。
    
- **Task Agent**：分析user agent提出的任务，调用工具，完成任务
    
- **Tool Simulator**：执行工具调用并提供真实的反馈。在每次工具执行后维护和更新状态，从而实复杂的multi-turn交互。它引入了一定的随机性来产生各种结果，包括成功、部分失败和边缘情况。
    
- **Judge Agent**：根据 Rubric 评估轨迹是否满足标准，仅保留高质量轨迹


---

### 3.4 真实沙盒补充：增强真实互动信号

虽然仿真提供了不错的可扩展性，但与真实环境还是有gap，尤其是代码，软件工程这种场景中。为此这里还搭建了真实的执行沙箱来补足模拟环境。这些沙箱负责执行实际代码，与真实的开发环境交互，并通过测试通过率等客观指标提供真实反馈。这种组合能极大减少仿真环境带来的误判等问题。
通过混合仿真与真实沙箱，生成了**多样化、高质量的工具使用轨迹**，平衡了覆盖范围和真实性。**规模化与自动化**的合成数据生成流程，结合**真实执行环境提供的对齐**，通过高质量的筛选流程，实质上实现了一种**大规模拒绝采样机制**。这些经筛选的**高质量合成数据**在用于监督微调时，显著提升了模型在**真实世界应用场景中的工具使用能力**，覆盖范围广泛，效果显著。

---

## 4. 强化学习训练阶段

基于 K1.5的工作，继续在 K2 中拓展了RL训练的任务多样性和训练资源 。Kimi K2 的强化学习策略系统结合可验证奖励 verifiable reward，并在比较主观的任务中引入 self-critic 奖励，覆盖从数学推理到复杂任务执行的多样场景。

### 4.1 Verifiable Rewards Gym：可验证奖励

这部分面向具备明确客观评价指标的任务类型，为不同任务设计了一套不同的“可打分”的训练场景，覆盖以下几个领域：
##### ✅数学、STEM 与逻辑推理任务：
对于这类问题，筛选原则为：
- **多样化的覆盖范围**。对于数学和词干任务，我们结合专家注释、内部QA和开源数据集来收集高质量的QA对。在收集过程中，我们利用标记系统有意增加覆盖率不足的域的覆盖率，包括结构化数据任务（例如，多跳表格推理、跨表聚合）和逻辑谜题（例如，24 局、数独、谜语、密码和摩尔斯电码解码）
- **难度中等**。既不能太简单也不能太难，这两种方法都可能产生很少的信号并降低学习效率。我们使用 SFT 模型的pass@k准确性来评估每个问题的难度，并仅选择中等难度的问题。

---

##### ✅ 复杂的指令执行任务：

这类任务要求模型不仅需要理解显式约束，还需要了解隐式约束、处理边缘情况以及在扩展对话中保持一致性。这里采用双路径系统来确保精度和稳健性：

- **混合验证策略**：
    
    - 若输出可被程序验证 → 使用代码执行器验证（e.g., 是否包含 3 条 bullet point）
        
    - 若为复杂语言条件 → 使用 LLM-as-a-judge 来判断
        
- **防欺骗机制**：
    
    - 加入 hack-check 层，检测模型是否“口头承诺”满足指令但行为不一致
        
这里使用的数据分三种方式生成：

1. 人类专家设计的复杂 prompt + rubric
    
2. AutoIF 启发式增强器生成边缘失败任务
    
3. 专用 LLM 模型自动构造边缘与多条件 prompt

---

##### ✅ 事实一致性（Faithfulness）奖励：

事实一致性对于agentic model模型至关重要，尤其是在多轮工具调用、自主生成推理链以及开放环境交互等场景中。受 FACTS Grounding 框架方法启发，训练了一个句子级别事实一致性评估模型，作为强化学习 reward model 使用。

---

##### ✅ 编码与软件工程任务：

强化学习任务包括：

- **竞赛级编程题**：
    
    - 使用开放数据集 + 合成题目
        
    - 每题都有真实测试用例作为 reward signal
        
- **软件开发场景**：
    
    - 使用 GitHub 收集真实 PR + issue + test case
        
    - 基于 Kubernetes 构建真实沙盒，支持并发 1 万实例
        
    - 用测试通过率作为主要奖励信号
        

---

##### ✅ 安全性 RL 任务：

最开始，设计了一组由人工精心设计的**种子提示集（seed prompts）**，这些提示涵盖了常见的高风险类别，如**暴力、诈骗与歧视**等。为了模拟更复杂的攻击（如角色扮演、文学叙述、学术语境等方式），我们设计了一套自动化的**提示词进化流水线**，由以下三个关键组件组成：

-  **攻击模型（Attack Model）**：以迭代方式生成针对目标模型的对抗性提示，旨在诱导其输出不安全内容
    
- **目标模型（Target Model）**：对上述提示做出响应，用以模拟潜在的安全漏洞
    
- **判别模型（Judge Model）**：评估交互过程，判断对抗性提示是否成功绕过了安全机制
    
每轮交互都会基于任务定制的评分标准（rubric）进行评估，由判别模型给出**是否越狱成功的二元判断标签（成功/失败）**

---

### 4.2 自我批判奖励（Self-Critique Rubric Reward）

为了将模型扩展到具有可验证奖励的任务之外，比如一些类似于创作，推理的主观任务，K2引入了自我批判奖励，旨在通过将从可验证场景中学到的能力扩展到更广泛的主观任务，使LLM在乐于助人、创造力、推理深度、事实性和安全性等方面与人类对齐。

- **Critic 模型**：用来根据rubric提供奖励信号，为了使其在RL训练初期就具备一定的“审美”和判断标准，团队在 SFT阶段，使用了开源和自有的偏好数据集对其进行训练来初始化
    
- **Actor 模型**：对广泛的测试用例生成响应，根据critic返回的reward来更新
    
- **Rubric 设计**：主要包含三个方面
	- 核心价值：比如清晰度、相关性、对话流畅性、客观地互动等
	    
	- 规范性标准：比如不靠冗长骗分、开头表示肯定和赞美、明确指出为什么自己的回复是符合用户要求的等
	    
	- 特定任务下的人工制定规则

为了不断提高critic的打分水平，需要不断更新critic，更新流程大致为
1. Actor 基于当前policy生成多个候选答案

2. 用可验证奖励机制给每个答案打分（比如正确得1分，错误得0分）

3. Critic 学习用 pairwise ranking（成对比较）的方式，预测哪个答案更好，损失函数用对比损失

	- 例如，给定两个答案A和B，奖励信号告诉我们A更好，Critic就要学会判断A优于B

4. 用这些有明确奖励信号的数据训练 Critic，优化其参数，使其评价标准与奖励信号一致，这种打分能力会隐式地迁移到为一些比较主观的问题打分上去
5. 注意：整个训练过程是混合的：有一部分数据有明确奖励，另一部分数据奖励信号较弱或主观，这样critic会随着actor的更新变化
---

### 4.3 RL 训练策略优化
K2 RL 的核心损失函数继承了K1.5中的策略优化思想，先回顾最初的强化学习目标：
$$
\max_\theta \; \mathbb{E}_{(x,y^*)\sim D,\,(y,z)\sim\pi_\theta}\bigl[r(x,y,y^*)\bigr].
$$

由于直接优化上述稀疏奖励信号难以收敛，并且我们希望每次更新不会偏离前一轮模型太远，引入**KL正则化**，得到：
$$
\max_{\theta}\; \mathbb{E}_{(x,y^*)\sim D}\Bigl[\;
  \mathbb{E}_{(y,z)\sim\pi_\theta}\bigl[r(x,y,y^*)\bigr]
  - \tau\,\mathrm{KL}\bigl(\pi_\theta(\cdot\mid x)\,\|\,\pi_{\theta_i}(\cdot\mid x)\bigr)
\Bigr],
$$
- 其中 $\pi_{\theta_i}$ 是第 $i$ 轮参考策略；  
- $\tau>0$ 控制正则化强度。
固定 $\pi_{\theta_i}$后，可得每个输入 $x$的最优分布
$$
\pi^*(y,z\mid x)
=
\frac{\pi_{\theta_i}(y,z\mid x)\,\exp\!\bigl(r(x,y,y^*)/\tau\bigr)}{Z(x)},
$$
其中
$$
Z(x)
=
\sum_{y',z'} \pi_{\theta_i}(y',z'\mid x)\,\exp\!\bigl(r(x,y',y^*)/\tau\bigr).
$$

两边取对数，得
$$
r(x,y,y^*) - \tau\log Z(x)
=
\tau\,\log\frac{\pi^*(y,z\mid x)}{\pi_{\theta_i}(y,z\mid x)}.
$$
由于动作空间十分大，直接优化闭式解不可行，转而定义均方误差代理损失：
$$
L(\theta)
=
\mathbb{E}_{(x,y^*)\sim D}\Bigl[
  \mathbb{E}_{(y,z)\sim\pi_{\theta_i}}\bigl[
    \bigl(r(x,y,y^*) - \tau\log Z(x)
      - \tau\log\tfrac{\pi_\theta(y,z\mid x)}{\pi_{\theta_i}(y,z\mid x)}
    \bigr)^2
  \bigr]
\Bigr].
$$
进一步，**近似 $\tau\log Z(x)$**：从 $\pi_{\theta_i}$ 采样 $k$ 条轨迹 $(y_j,z_j)$，估计
  $$
  \tau\log Z(x)\approx \tau\log\Bigl(\tfrac1k\sum_{j=1}^k e^{r(x,y_j,y^*)/\tau}\Bigr).
  $$
为了降低方差，用采样奖励的均值作为经验奖励基线
  $$
  r = \tfrac1k\sum_{j=1}^k r(x,y_j,y^*)
  $$
代替$\tau\log Z(x)$，就得到了K2的形式：

$$

L_{RL}(\theta) = \mathbb{E}_{x \sim \mathcal{D}} \left[ \frac{1}{K} \sum_{i=1}^K \left( r(x, y_i) - \bar{r}(x) - \tau \log \frac{\pi_\theta(y_i|x)}{\pi_{old}(y_i|x)} \right)^2 \right]

$$

---

随着扩展 RL 训练以涵盖 K2 中更广泛的任务，一个主要挑战是在所有领域实现一致的性能改进。为了解决这个问题，我们对 RL 算法进行了一些补充。

#### ✅ 推理预算控制（Budget Control）

  

- **动机**：防止 RL 训练导致回复无谓变长，鼓励模型在预算内生成高效、简洁的解答，提升 token 利用率提升推理效率。

- **实现**：

    - 每个样本设定最大 token 预算（根据任务类型动态调整）。

    - 超出预算的回复会被截断，并在奖励中施加惩罚项。

> **示例**：  
> - 复杂推理任务允许更高预算，简答任务预算较低。
> - 超预算回复奖励 = 原奖励 - 罚分。

  

#### ✅ 高质量数据保持损失（PTX Loss）

  

- **动机**：为了防止 RL 训练遗忘高质量知识，不会过拟合 RL 任务，能在更多领域保持高质量输出。

- **实现**：

    - 精选高质量样本，构建 PTX 数据集。

    - 在 RL 训练中引入一个辅助损失函数 PTX ，持续优化模型在这些数据上的表现。  

> **公式**：  
> 总损失 = RL 损失 + λ × PTX 损失， 其中 λ 为权重超参数。

  

#### ✅ 温度衰减（Temperature Decay）

  

- **动机**：兼顾训练初期的探索和后期的收敛。

- **实现**：

    - 训练初期采用高采样温度，促进多样性和创新。

    - 随训练进展逐步降低温度，最终收敛到稳定输出。

  

> **示例**：  
> - 初期温度 1.0，逐步线性/指数衰减至 0.7 或更低。
    

---

## 5. 基础设施支持：面向 Agentic 的 RL 训练系统设计

### 5.1 混合协同架构

  

本系统采用了类似 K1.5 的混合协同架构，实现了同步强化学习训练。在该架构下，训练引擎和推理引擎共存于同一工作节点。当某一引擎处于活跃状态时，另一引擎会释放或卸载其 GPU 资源以让出计算能力。每轮 RL 训练迭代中，中央控制器首先调用推理引擎生成新的训练数据，随后通知训练引擎对新数据进行训练，并将更新后的参数发送给推理引擎以备下轮迭代使用。

  

为提升吞吐量，每个引擎都经过了深度优化。随着模型规模扩展至 K2 级别，引擎切换延迟和故障恢复的影响愈发显著，系统在这些方面也做了专门设计。

  

### 5.2 高效引擎切换

  

在 rollout 阶段，训练引擎的参数被卸载到 DRAM，重新激活时只需一次 H2D 传输。推理引擎因参数分片方式不同，激活更为复杂，需要获取最新参数。

针对 K2 规模，传统网络文件系统无法满足参数重分片和广播的带宽需求。为此，系统设计了分布式 checkpoint 引擎，各节点本地获取参数副本并全量广播，推理引擎仅获取所需分片。1T 级模型参数更新采用流水线方式，降低内存占用。
![Pasted image 20250723101318.png](/img/user/Pasted%20image%2020250723101318.png)
虽然该方法传输数据量略大，但大幅简化了系统设计，降低了引擎耦合度，便于维护和测试。实际测试显示，该方案同步开销低、带宽利用率高，Kimi K2 的参数更新可在 30 秒内完成，对 RL 训练迭代影响可忽略。

  

### 5.3 高效系统启动

  

大规模训练易受系统故障影响，因此优化启动时间尤为关键。训练引擎启动时，每个训练节点只需选择性地从磁盘读取部分或全部参数，并将必要参数广播给其他节点，确保所有节点只需一次性读取 checkpoint，最大限度减少磁盘 IO。

  

推理引擎作为独立副本，系统避免了在其间引入额外同步屏障。启动时，复用 checkpoint 引擎，由其集体从磁盘读取 checkpoint，并按前述方法更新未初始化的推理引擎状态。该设计还提升了系统的容错性，单个推理副本可在无需与其他副本通信的情况下独立重启。

  

### 5.4 智能体式 Rollout

  

支持长时序、多轮次的智能体任务训练，针对环境交互复杂和 rollout 持续时间长等挑战，系统进行了如下优化：

- 对于需等待环境反馈（如虚拟机、代码解释器）导致 GPU 空闲的问题，通过将重型环境部署为可扩展的独立服务，并大规模并发 rollout，有效提升了 GPU 利用率，降低了高延迟交互的影响。

- 针对单次 rollout 轨迹过长，采用 partial rollout 技术，允许长尾任务暂停并在后续迭代中恢复，避免阻塞整体进程。

- 设计了类 OpenAI Gym 的统一接口，简化新环境的集成流程，便于未来扩展至更多交互式环境。

---

## 6. 评估结果：Agentic 能力的量化提升

Kimi K2 在多个 agentic benchmark 上达到 SOTA：

### 6.1 评测设置

  

**评测基准**  

Kimi-K2-Instruct 在多个领域进行了评测：

  

- **代码能力**：LiveCodeBench v6（2024年8月至2025年5月题目）、OJBench、MultiPL-E、SWE-bench Verified、TerminalBench、Multi-SWE-bench、SWE-Lancer、PaperBench、Aider-Polyglot。

- **工具使用**：τ 2-Bench、AceBench，重点考察多轮工具调用能力。

- **推理能力**：AIME 2024/2025、MATH-500、HMMT 2025、CNMO 2024、PolyMath-en、ZebraLogic、AutoLogi、GPQA-Diamond、SuperGPQA、Humanity’s Last Exam (Text-Only)。

- **长上下文能力**：MRCR4（长上下文检索）、DROP、FRAMES、LongBench v2（长上下文推理）。

- **事实性**：FACTS Grounding、Vectara Hallucination Leaderboard、FaithJudge。

- **通用能力**：MMLU、MMLU-Redux、MMLU-Pro、IFEval、Multi-Challenge、SimpleQA、LiveBench（截至2024-11-25）。

  

**对比基线**  

我们对比了开源和专有前沿模型，所有模型均在“非思考”模式下评测，排除推理时计算带来的额外增益。

  

- 开源基线：DeepSeek-V3-0324、Qwen3-235B-A22B（按官方推荐的 no-thinking 配置）。

- 闭源基线：Claude Sonnet 4、Claude Opus 4、GPT-4.1、Gemini 2.5 Flash Preview（2025-05-20），均通过官方 API 以统一的 temperature 和 top-p 设置调用。

  

**评测配置**  

- 所有模型均在非思考模式下调用。

- 输出 token 长度上限为 8192（SWE-bench Verified（Agentless）为 16384）。

- 针对高方差基准，采用重复采样 k 次并取均值（Avg@k）。

- 长上下文任务 context window 设为 128K tokens，超出部分截断。

- SWE-bench Verified 评测两种模式：Agentless Coding（单次 patch，无测试）和 Agentic Coding（bash/editor 工具，单次和多次尝试，后者采用 best-of-N 策略和内部验证器）；SWE-bench Multilingual 仅用单次 agentic 设置。


  

### 6.2 评测结果

  

Kimi-K2-Instruct 的综合评测结果见后表，以下为四大核心领域的亮点：

  

#### 智能体与竞赛编程

  

Kimi-K2-Instruct 在真实 SWE 任务上展现了开源 SOTA 性能，SWE-bench Verified（65.8%，多次尝试 71.6%）、SWE-bench Multilingual（47.3%）、SWE-lancer（39.1%）均大幅缩小与 Claude 4 Opus 和 Sonnet 的差距。在竞赛编程基准（如 LiveCodeBench v6 53.7%，OJBench 27.1%）上同样领先，展现了跨难度的实用编程能力。

  

#### 智能体工具使用

  

在多轮工具使用基准上，Kimi-K2-Instruct 设立了新标杆：τ 2-Bench Pass@1 达 66.1，ACEBench 达 76.5，均大幅超越所有基线，体现了其在多领域、可控、智能体驱动工具编排上的优势。

  

#### 通用能力

  

Kimi-K2-Instruct 在通用知识、数学、指令跟随和长上下文任务上表现均衡且强劲。SimpleQA（31.0%）、MMLU（89.5%）、MMLU-Redux（92.7%）均超越开源同类，IFEval（89.8%）、Multi-Challenge（54.1%）在指令类基准上领先。数学与 STEM 领域同样表现优异（AIME 2024: 69.6%，GPQA-Diamond: 75.1%），在长上下文事实性和检索任务中也具竞争力（DROP: 93.5%，MRCR: 55.0%）。这些结果表明 Kimi-K2-Instruct 是一款全能型模型，适用于短、长上下文场景。

  

#### 开放式评测

  

在 LMSYS Arena 榜单（2025年7月17日），Kimi-K2-Instruct 以 3000+ 用户投票成为开源模型第一、总榜第五。该真实世界偏好信号（基于多样化盲测提示）进一步印证了 Kimi-K2 在开放式任务中生成高质量响应的能力。

![Pasted image 20250723102255.png](/img/user/Pasted%20image%2020250723102255.png)

---

## 7. 总结与启示

Kimi K2 的后训练体系展示了未来 Agentic 大模型的范式：

- 数据：合成+真实结合，规模大、覆盖广、评估细
    
- 方法：结合 RL + 自我评估，构建持续自我改进系统
    
- 架构：高性能、可扩展、容错性强的训练/推理系统
    
- 成果：在多项任务上接近或超越闭源模型
    

Kimi K2 为构建下一代自主智能体提供了坚实的工程与方法论基础。

---
