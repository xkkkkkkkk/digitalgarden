---
{"dg-publish":true,"permalink":"/论文解读/算法解读/FR3E：基于熵引导探索的大语言模型强化学习/","title":"FR3E：基于熵引导探索的大语言模型强化学习","tags":["gardenEntry"]}
---


**摘要**  
> First Return, Entropy-Eliciting Explore（FR3E）引入了一种用于可验证奖励强化学习（RLVR）微调LLM的探索框架该方法利用基于token的熵来识别长推理过程中不确定性强的部分，从而实现有针对性的局部推理，这使得更训练过程更稳定，在数学基准中也表现出更好的的推理能力
## 1. 引言：问题背景与研究动机

  
### 💡 核心问题

当前LLM在强化学习（RLVR）中的关键挑战是**稀疏奖励下的探索效率**和**长期信用分配**问题。传统方法如PPO或GRPO使用最终结果奖励来估计所有中间动作的价值，这种均匀信用分配无法区分关键推理步骤和不重要动作，导致学习效率低下。

  

### 🔍 现有局限

1. **价值模型方法**（如PPO/VAPO）：在LLM巨大状态空间中训练稳定的critic模型极其困难

2. **轨迹级方法**（如GRPO）：缺乏细粒度信用分配，无法识别关键推理步骤

3. **启发式中间奖励**（如VinePPO/PRMs）：依赖启发式规则或昂贵的人工标注，缺乏适应性

  

### 🚀 研究思路

- **用于轨迹级奖励塑造的可靠探索框架**：FR3E 提出了一种新颖的强化学习算法，通过强调可靠的探索路径来改进奖励塑造。它识别推理轨迹中不确定性高的决策点，并执行有针对性的试运行，以构建语义上接地的中间反馈，而无需密集的监督。
- **改进的训练稳定性和推理能力**：该框架在训练过程中保持熵的稳定和逐步增加，防止了探索的过早收敛，并使模型能够生成更长、更连贯的响应。这对于像 Qwen-Math-7B 这样的专业模型尤其有利。
- **更稳健的积极奖励信号**：FR3E 通过鼓励围绕高熵状态的结构化探索来生成更多积极的奖励信号。实验结果表明，完全正确轨迹的比例更高，而完全错误轨迹的比例更低。


---

## 2. 问题描述：

### 2.1 自回归生成的马尔可夫决策过程（MDP）建模

为了应用强化学习来优化大型语言模型LLM，论文首先将LLM的自回归文本生成过程形式化为一个马尔可夫决策过程（MDP）。这是一个关键的理论步骤，它为使用RL术语和算法提供了框架。


MDP 由一个五元组 $(S, A, P, r, \gamma)$ 定义：

- **状态 (States, S)**：在 LLM 的上下文中，一个状态 $s_t$ 被定义为到时间步 $t$ 为止的所有文本序列。这包括初始的输入提示（prompt） $x_0,...,x_m$ 和模型已经生成的词元（tokens） $y_0,...,y_t$。因此，$s_t = (x_0,...,x_m, y_0,...,y_t)$。状态完整地捕捉了决策所需的全部历史信息
    
- **动作 (Actions, A)**：一个动作 $a$ 对应于从模型的词汇表 $V$ 中选择下一个词元 $y_{t+1}$。因此，动作空间就是整个词汇表的大小
    
- **转移动态 (Dynamics, P)**：在自回归生成中，状态转移是**确定性的**。给定当前状态 $s_t$ 和选择的动作 $a_t = y_{t+1}$，下一个状态 $s_{t+1}$ 是唯一确定的，即 $s_{t+1} = (x_0,...,x_m, y_0,...,y_{t+1})$。随机性不来源于环境的转移，而是来源于策略 $\pi$ 选择动作的不确定性
    
- **奖励 (Reward, r)**：在复杂的推理任务中，奖励通常是**稀疏的 (sparse)**。这意味着只有在整个序列生成完毕后，系统才能根据最终答案的正确性给予一个一次性的奖励（例如，正确为 $+1$，错误为 $0$）。在中间的每一步生成过程中，都没有奖励信号。这种稀疏性是 RL 在 LLM 中面临的核心挑战，因为它导致了“信用分配难题”——难以判断长序列中的哪一步对最终结果贡献最大。
    
- **折扣因子 (Discount Factor, γ)**：一个介于 $(0, 1)$ 之间的值，用于平衡即时奖励与未来奖励的重要性。

**目标函数**：RL的目标是学习一个策略 `π`，该策略将状态映射到动作的概率分布，以最大化期望累积奖励。其数学表达式为：
$$
\mathbb{E}_{\pi}\left[\sum_{t} \gamma^t r(s_t, a_t)\right]
$$
通过将LLM的生成过程构建为MDP，论文为后续引入和分析RL算法奠定了基础。

### 2.2 用于数据筛选的拒绝采样

在强化学习训练中，如果一个批次（batch）中的数据不能提供有效的学习信号，训练效率会大大降低。在训练过程中，模型对于某些提示，可能在多次生成（rollouts）中始终得到完全相同的奖励，要么全部正确（奖励全为1），要么全部错误（奖励全为0）。
*   **负面影响**：当这种情况发生时，用于计算策略梯度的优势估计（advantage estimates）会变为零。零优势意味着没有梯度信号，模型无法从这些样本中学习如何调整其策略。这不仅浪费了计算资源，还导致有效批量大小（effective batch size）减小，从而增加了梯度估计的方差，可能导致训练不稳定。
*   **解决方案**：为了缓解此问题，采用了**拒绝采样**。类似DAPO的动态采样，具体操作如下：
    1.  针对每一个提示，执行多次独立的生成过程。
    2.  计算每个生成轨迹的二元奖励（0或1）。
    3.  **如果一个提示产生的所有轨迹的奖励完全相同，则该提示被视为“退化”样本，并从当前的训练批次中丢弃（拒绝）。**

### 2.3 鼓励探索的非对称裁剪

标准的PPO算法为了防止策略更新过大导致训练崩溃，引入了裁剪（clipping）机制。然而，传统的对称裁剪在稳定训练的同时，也限制了模型的探索能力。

*   **Clip-Higher机制**：为了解决这个问题并明确鼓励探索，论文采用了Clip-Higher机制，这是一种**非对称裁剪**，同样也是参考了DAPO。其目标函数如下：
$$
\mathcal{L}^{PPO}(\theta) = \sum \min\left(r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}(r_{i,t}(\theta), 1 - \epsilon_{\text{low}}, 1 + \epsilon_{\text{high}})\hat{A}_{i,t}\right)
$$
*   **技术优势**：这种非对称设计在不牺牲训练稳定性的前提下，**系统性地鼓励模型去探索那些低概率但可能正确的推理路径**。它为策略提供了更大的向上调整空间，有助于维持策略的多样性（高熵），从而在长时程推理中实现更有效的探索。

---


## 3. 技术方法

  

### 3.1 整体设计思路

FR3E 的核心是将复杂的探索问题分解为两个更易于管理的部分：
![Pasted image 20250730233302.png](/img/user/Pasted%20image%2020250730233302.png)

*   **首次返回 (First Return)**：此阶段的核心任务是**识别**。它首先生成一条完整的推理轨迹，然后通过分析模型自身的熵（表征不确定性），来识别出这条轨迹上最关键的决策点或“推理分叉口”。
*   **熵引导探索 (Entropy-Eliciting Explore)**：此阶段的核心任务是**探索**。它利用第一阶段识别出的关键决策点作为“锚点”，从这些点出发进行多样化的局部探索，以评估这些决策点的潜在价值。

这种结构化的方法通过将熵信号直接整合到路径选择和策略更新中，优化了传统的探索-利用权衡。

  

### 3.2 阶段一：首次返回 (First Return) - 通过不确定性信号定义区块

对于给定的训练查询 $q$，首先使用当前的策略 $π_θ$ 生成一个完整的、作为基准的推理轨迹 $P_{base}$

$$
P_{\text{base}} = (q, t_1, t_2, \ldots, t_L)
$$

其中，$t_i$ 是序列中的第 $i$ 个生成的词元（token），$L$ 是轨迹的总长度。

这个轨迹代表了模型从初始问题到最终输出的完整思考过程。为了识别轨迹中模型最不确定的位置，算法会计算每个词元位置 $k$ 的策略熵 $H_k$。在位置 $k$，策略的熵定义为：

$$
H_k = -\sum_{v \in V} \pi_\theta(v | q, t_{<k}) \log \pi_\theta(v | q, t_{<k})
$$
其中：

- $V$ 是整个词汇表。
    
- $\pi_\theta(v \mid q, t_{<k})$ 是在给定查询 $q$ 和已生成前缀 $t_{<k}$ 的条件下，模型预测下一个词元为 $v$ 的概率。
    

**高熵 $H_k$ 值表明模型在该位置对下一步的选择非常不确定，是进行探索的理想位置。** 因此在计算出整个轨迹所有位置的熵之后，算法会选择$k$个熵值最高的位置，形成一个熵敏感位置集合 $\mathcal{K}$：

$$
\mathcal{K} = \text{TopK}(\{H_k\}_{k=1}^L) = \{k_1, k_2, \ldots, k_K\}
$$

利用上述识别出的高熵位置 $\mathcal{K}$ 作为"断点"，将原始的 $P_\text{base}$ 分割成一系列语义上连贯的**区块 (Block)** $B_n$。

- 区块 $B_n$ 定义为第 $n-1$ 个和第 $n$ 个高熵位置之间的词元序列
    
- 基于这些区块，可以定义一系列**中间推理状态 (Intermediate Reasoning States)** $S_j$
    
    $$Sj=(q,B1,B2,…,Bj)Sj​=(q,B1​,B2​,…,Bj​)$$
    
状态 $S_j$ 代表了截至第 $j$ 个区块（即第 $j$ 个高熵位置之前）的完整推理前缀，这种基于熵的分割方法为后续的局部策略优化和精准的信用分配提供了结构化基础。


### 3.3 阶段二：熵引导探索 - 从锚点进行多样化路径采样

在确定了中间推理状态（锚点）$S_j$ 后，此阶段开始进行有针对性的探索。

1. **多样化采样**：对于每一个中间状态 $S_j$，算法会重置上下文并生成 $M$ 个不同的后续轨迹（rollouts），记为 $\{Y_{j,m}\}_{m=1}^M$
    $$
    Y_{j,m} \sim \pi_\theta(\cdot | S_j)
    $$

2.  **奖励定义**：每个完整的轨迹（$S_j$ + $Y_{j,m}$）根据其最终答案是否正确，被赋予一个二元奖励 $r_{j,m}$
    $$
    r_{j,m} = \begin{cases} 1 & \text{if } Y_{j,m} \text{ is correct} \\ 0 & \text{otherwise} \end{cases}
    $$

3.  **状态价值估计**：利用这 $M$ 次采样的平均奖励，可以估计出中间状态 $S_j$ 的**经验价值 (Empirical Value)** $V(S_j)$
    $$
    V(S_j) = \frac{1}{M} \sum_{m=1}^{M} r_{j,m}
    $$
    $V(S_j)$ 量化了从前缀 $S_j$ 出发能够达成最终正确答案的平均概率。它为模型提供了一个密集的、关于推理路径质量的反馈信号，有效地将稀疏的最终奖励转化为了对中间步骤的评估。

### 3.4 自适应优势调制以实现稳定学习

为了动态地调整学习信号，算法引入了一个**优势调制因子 (Advantage Modulation Factor)** $α_j$

$$
\alpha_j = \frac{1}{\exp(V(S_j) - V(S_{j-1}))}
$$

这个调制因子的作用像一个动态反馈控制器：
*   **当推理向更好发展时 ($V(S_j) > V(S_{j-1})$)**:
    -   $α_j < 1$，它会**减小**对成功路径的优势信号。这可以防止模型过早地收敛到某条好的路径上（过拟合），从而保留了继续探索其他可能性的多样性。
*   **当推理停滞或倒退时 ($V(S_j) ≤ V(S_{j-1})$)**:
    -   $α_j ≥ 1$，它会**放大**优势信号。这鼓励模型进行更激进的探索，以摆脱当前的推理瓶颈。

最终，用于策略更新的**调制后优势 (Modulated Advantage)** $A'$ 被定义为：
$$
A'(S_j, P_{j,m}) = \alpha_j \cdot A(S_j, P_{j,m})
$$
这种自适应机制使得策略梯度估计器在理论上近似无偏，同时有效地利用了通过结构化探索获得的密集反馈，最终实现了更稳定和高效的学习过程。

---

## 4. 实验评估与结果分析

  

### 4.1 实验设计

- **数据集**：混合DeepScaler（基础）和SimpleRL 3-5级（挑战）

- **评估基准**：GSM8K、Math500、AIME24等7个数学推理基准

- **对比方法**：GRPO++

- **模型**：Qwen2.5-7B/Math-7B/32B

- **关键设置**：batch=512, lr=1e-6, 每个prompt 16个rollout

  

### 4.2 核心结果

![Pasted image 20250731001247.png](/img/user/Pasted%20image%2020250731001247.png)
![Pasted image 20250731001328.png](/img/user/Pasted%20image%2020250731001328.png)  

**主要发现**：

1. **熵保持能力**：FR3E在训练后期仍保持较高熵，避免早熟收敛

2. **性能提升**：在Qwen2.5-32B上AIME24准确率提升6.1%

3. **轨迹质量**：显著增加"All-Right"轨迹比例，减少"All-Wrong"轨迹比例

4. **长度控制**：生成更长的连贯推理链

  
  

---

  

## 5. 核心贡献与技术价值

  

### 5.1 主要贡献

1. **理论贡献**：将Go-Explore原则适配到自回归生成，提出熵引导的探索机制

2. **技术贡献**：

   - 基于不确定性的轨迹分割方法

   - 无价值模型的局部rollout策略

   - 自适应优势调制算法

3. **应用贡献**：在数学推理任务中实现更稳定训练和更长推理链

  

### 5.2 对领域的影响

- 为RL4LM提供了新的探索范式

- 证明模型内在不确定性可作为有效的探索信号

- 推动对"何时探索"而非"如何探索"的研究方向

  

